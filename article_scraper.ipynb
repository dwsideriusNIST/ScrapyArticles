{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import scrapy\n",
    "from urllib.request import urlopen #to open the urls that the dois are put into\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "#for the ArticleItem section\n",
    "from scrapy.item import Item, Field\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.spiders import Spider\n",
    "\n",
    "#for the spiders \n",
    "from scrapy import Spider\n",
    "from scrapy.http import TextResponse #defines what response is in xpath\n",
    "\n",
    "#to run the spider in Jupyter notebook, have to restart the kernel each time to run it\n",
    "from scrapy.settings import Settings\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "#Running spiders imports \n",
    "from twisted.internet import reactor\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.log import configure_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://pubs.acs.org/doi/full/10.1021/ja302991b\n",
      "http://dx.doi.org/10.1016/j.micromeso.2012.01.033\n",
      "http://link.springer.com/article/10.1007/s10450-012-9423-1\n",
      "wrong publisher\n",
      "http://link.springer.com/article/10.1007/s10450-013-9527-2\n"
     ]
    }
   ],
   "source": [
    "#doi_list = [ACS, Elsevier, Springer, Aiche Journal]\n",
    "#automatically decides which journal a doi is for, then it will follow a certain set of instructions\n",
    "doi_list = [\"10.1021/ja302991b\", \"10.1016/j.micromeso.2012.01.033\", \"10.1007/s10450-012-9423-1\", \"10.1002/aic.690470520\", \"10.1007/s10450-013-9527-2\"]\n",
    "for d in doi_list:\n",
    "    test_url = 'http://dx.doi.org/{0}'.format(d)\n",
    "    \n",
    "    headers = {'Accept': 'application/citeproc+json'}\n",
    "    bib_info = json.loads(requests.get(test_url, headers=headers).content)\n",
    "    if bib_info['publisher'] == 'American Chemical Society (ACS)':\n",
    "        \n",
    "        doi_acs = bib_info.get('DOI')\n",
    "        full_url_acs = 'http://pubs.acs.org/doi/full/{0}'.format(doi_acs)\n",
    "        print(full_url_acs)\n",
    "        response_acs = urlopen(full_url_acs)\n",
    "        content_acs = response_acs.read()\n",
    " \n",
    "        \n",
    "    elif bib_info['publisher'] == 'Elsevier BV':\n",
    "        \n",
    "        doi_el = bib_info.get('DOI')\n",
    "        full_url_el = 'http://dx.doi.org/{0}'.format(doi_el)\n",
    "        print(full_url_el)\n",
    "        response_el = urlopen(full_url_el)\n",
    "        content_el = response_el.read()\n",
    "        \n",
    "    elif bib_info['publisher'] == 'Springer Nature':\n",
    "        \n",
    "        doi_spr = bib_info.get('DOI')\n",
    "        full_url_spr = 'http://link.springer.com/article/{0}'.format(doi_spr)\n",
    "        print(full_url_spr)\n",
    "        response_spr = urlopen(full_url_spr)\n",
    "        content_spr = response_spr.read()\n",
    "        \n",
    "    else:\n",
    "        print('wrong publisher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#You define classes that you want the spider to scrape\n",
    "#This is what I defined as wanting from each article\n",
    "#later I will create the spider and tell the spider where to find this for each publisher website\n",
    "\n",
    "class ArticleItem(Item):\n",
    "    title = Field()\n",
    "    authors = Field()\n",
    "    doi = Field()\n",
    "    abstract = Field()\n",
    "    text = Field()\n",
    "    figures = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables and lists defined to add urls for each publisher\n",
    "full_url_acs = ''\n",
    "full_url_spr = ''\n",
    "full_url_acs_lst = []\n",
    "full_url_spr_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding out which publisher each DOI comes from\n",
    "#Two currently defined are ACS and Springer Nature\n",
    "#Sort DOIs, under each if statement the corresponding spider for each publisher\n",
    "#to change the list of DOIs => change file in Bash loop not in this code\n",
    "\n",
    "dois = open('doi_list.txt') #doi_list.txt is the where the list of DOIs came from\n",
    "doi_lst = dois.readlines()\n",
    "fixed_doi = []\n",
    "for x in doi_lst:\n",
    "    fixed_doi.append(re.sub('\\n','', x))\n",
    "doi_lst = fixed_doi\n",
    "\n",
    "for d in doi_lst:\n",
    "    test_url = 'http://dx.doi.org/{0}'.format(d)\n",
    "    \n",
    "    headers = {'Accept': 'application/citeproc+json'}\n",
    "    bib_info = json.loads(requests.get(test_url, headers=headers).content)\n",
    "\n",
    "    if bib_info['publisher'] == 'American Chemical Society (ACS)':\n",
    "        \n",
    "        doi_acs = bib_info.get('DOI')\n",
    "        full_url_acs = 'http://pubs.acs.org/doi/full/{0}'.format(doi_acs)\n",
    "        response_acs = urlopen(full_url_acs)\n",
    "        content_acs = response_acs.read()\n",
    "\n",
    "        full_url_acs_lst.append(full_url_acs)\n",
    "        \n",
    "    \n",
    "        \n",
    "    elif bib_info['publisher'] == 'Springer Nature':\n",
    "        \n",
    "        doi_spr = bib_info.get('DOI')\n",
    "        full_url_spr = 'http://link.springer.com/article/{0}'.format(doi_spr)\n",
    "        response_spr = urlopen(full_url_spr)\n",
    "        content_spr = response_spr.read()\n",
    "\n",
    "        full_url_spr_lst.append(full_url_spr)\n",
    "\n",
    "    else:\n",
    "        print('wrong publisher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Spider for ACS \n",
    "class ArticleSpider(scrapy.Spider):\n",
    "    name = 'ArticleSpider' #Name the spider anything \n",
    "    allowed_domains = [\"http://pubs.acs.org/\"]\n",
    "    start_urls = full_url_acs_lst \n",
    "\n",
    "    #where to find the text, easy to find using inspect tool on webpage\n",
    "    #if text isn't scraped add \"/text()\" after class\n",
    "    #use items defined earlier in ArticleItem\n",
    "    def parse(self, response):\n",
    "        item = ArticleItem()\n",
    "        item['title'] = response.xpath('//span[@class=\"hlFld-Title\"]/text()').extract()\n",
    "        item['authors'] = response.xpath('//div[@id=\"authors\"]/text()').extract()\n",
    "        item['doi'] = response.xpath('//div[@id=\"doi\"]/text()').extract()\n",
    "        item['abstract'] = response.xpath('//p[@class=\"articleBody_abstractText\"]/text()').extract\n",
    "        item['text'] = response.xpath('//div[@class=\"hlFld-Fulltext\"]').extract()\n",
    "        item['figures'] = response.xpath('//img[@alt=\"figure\"]').extract()\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spider for Springer Nature\n",
    "class ArticleSpiderSpr(scrapy.Spider):\n",
    "    name = 'ArticleSpiderSpr'\n",
    "    allowed_domains = [\"https://link.springer.com\"]\n",
    "    start_urls = full_url_spr_lst\n",
    "\n",
    "    def parse(self, response):\n",
    "        item = ArticleItem()\n",
    "        item['title'] = response.xpath('//h1[@class=\"ArticleTitle\"]/text()').extract()\n",
    "        item['authors'] = response.xpath('//span[@class=\"authors__name\"]/text()').extract()\n",
    "        item['doi'] = response.xpath('//span[@id=\"doi-url\"]/text()').extract()\n",
    "        item['abstract'] = []\n",
    "        item['text'] = response.xpath('//div[@id=\"body\"]/descendant::text()').extract()\n",
    "        item['figures'] = response.xpath('//div[@class=\"MediaObject\"]').extract()\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to run the spiders\n",
    "#Bash loop will do this for each DOI and create a unique JSON file name for each\n",
    "def run():\n",
    "\n",
    "    settings = get_project_settings()\n",
    "    settings.set('FEED_FORMAT', 'jsonlines')\n",
    "    settings.set('FEED_URI', 'result6.jl')\n",
    "\n",
    "    configure_logging()\n",
    "    runner = CrawlerRunner(settings)\n",
    "\n",
    "    runner.crawl(ArticleSpider)\n",
    "    runner.crawl(ArticleSpiderSpr)\n",
    "\n",
    "    d = runner.join()\n",
    "    d.addBoth(lambda _: reactor.stop())\n",
    "\n",
    "    reactor.run()  # the script will block here until all crawling jobs are finished\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
